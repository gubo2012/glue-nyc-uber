{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC TLC Public Dataset Glue Example #\n",
    "\n",
    "This example analyzes [New York City Taxi and Limousine Commission Trip Record Data](https://registry.opendata.aws/nyc-tlc-trip-records-pds/) and prepares it for an example machine learning exercise. This example assumes that the NYC taxi dataset has already been crawled using a [Glue Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html) and the inferred schema is already stored in the Glue Catalog database.\n",
    "\n",
    "To get strated, first we import Python packages to create a Glue context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.utils import getResolvedOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Glue Context ###\n",
    "Next we create a Glue context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Glue context\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Glue Dynamic Frame ###\n",
    "In this example, we first create a Glue Dynamic Frame, and then get a PySpark DataFrame from the Glue Dynamic Frame. One can of course use the [Glue Dynamic Frame API](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html) for what we want to do, but we chose to use the PySpark DataFrame API, because presumably you may already be familiar with the PySpark API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DynamicFrame using the uber' table\n",
    "uber_dyf = glueContext.create_dynamic_frame.from_catalog(database=\"nyc-tlc-misc\", table_name=\"uber_nyc_data_csv\")\n",
    "uber_df=uber_dyf.toDF()\n",
    "print uber_df.printSchema()\n",
    "uber_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data ###\n",
    "Next, we clean the data. We drop any rows with any NULL on NaN data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up data \n",
    "# remove id column as we don't need it\n",
    "uber_df1=uber_df.drop(uber_df.id)\n",
    "\n",
    "# drop all rows with any null value\n",
    "uber_df1=uber_df.dropna(how='any')\n",
    "\n",
    "# filter rows where destnation, orign and trip duration are not set to NULL\n",
    "uber_df1=uber_df1.filter((uber_df1.destination_taz != 'NULL')  & \n",
    "    (uber_df1.origin_taz != 'NULL')  & \n",
    "    (uber_df1.trip_duration != 'NULL') )\n",
    "\n",
    "# show 10 rows\n",
    "uber_df1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PySpark user-defined functions ###\n",
    "Below, we import relevant Python clasess for defining PySpark user-defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, to_timestamp\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function for extracting ordinal day of the week from pickup date timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define UDF for extracting pickup day of the week from datetime\n",
    "\n",
    "def weekday(x):\n",
    "    pickup=datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "    return int(pickup.date().weekday())\n",
    "    \n",
    "pickup_day_udf = udf(weekday, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function for extracting month from the pickup date timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define month udf for extracting pickup month from datetime\n",
    "def month(x):\n",
    "    pickup=datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "    return int(pickup.date().month)\n",
    "    \n",
    "pickup_month_udf = udf(month, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function for extracting hour of the day from the pickup date timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define minutes udf for extracting pickup hour from datetime\n",
    "\n",
    "def pickup_time(x):\n",
    "    ptime = datetime.strptime(x, '%Y-%m-%d %H:%M:%S').time()\n",
    "    return int(ptime.hour)\n",
    "    \n",
    "pickup_time_udf = udf(pickup_time, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker built-in XGBoost algorithm expects numeric input. So, we need to encode the pickup source and destination target zones as numbers. We define a PySpark user-defined function that encodes source and target zones as hexadecimal integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_taz(x):\n",
    "   return int(x, 16)\n",
    "\n",
    "taz_udf=udf(encode_taz, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function that computes duration of the trip in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define duration udf for extracting duration in minutes\n",
    "def duration(x):\n",
    "    time=x.split(':')\n",
    "    duration = int(time[0]*60) + int(time[1])\n",
    "    return duration\n",
    "\n",
    "duration_udf = udf(duration, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for SageMaker XGBoost algorithm ###\n",
    "SageMaker XGBoost algorithm expects the label to be the first column. So, we transform the PySpark DataFrame to make 'duration' as the first column, because we want to train the model to predict duration. Other columns of the DataFrame are transformed using PySpark user-defined functions defined above. \n",
    "\n",
    "We also drop any rows with Null or NaN values as a result of transformations, and also drop any row with duration of 60 minutes or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data frame\n",
    "# we want trip duration (seconds) in the first column as label for the row\n",
    "# our feature vector includes origin, desination, and pickup month, day, and hour\n",
    "# we will discard other columms\n",
    "uber_df2 = uber_df1.select(duration_udf(uber_df1.trip_duration).alias('duration'),\n",
    "    taz_udf(uber_df1.origin_taz).alias('origin'), \n",
    "    taz_udf(uber_df1.destination_taz).alias('destination'), \n",
    "    pickup_month_udf(uber_df1.pickup_datetime).alias('month'), \n",
    "    pickup_day_udf(uber_df1.pickup_datetime).alias('day'), \n",
    "    pickup_time_udf(uber_df1.pickup_datetime).alias('pickup_time'))\n",
    "\n",
    "uber_df3 = uber_df2.dropna(how='any')\n",
    "uber_df4 = uber_df3.filter(uber_df3.duration < 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show \n",
    "uber_df4.show(10)\n",
    "uber_df4.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save prepared data in S3 bucket ###\n",
    "Finally, we save the transformed PySpark DataFrame in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save second data frame\n",
    "uber_df4.write.save(\"s3://aws-ajayvohra-nyc-tlc-misc/glue/output/uber_nyc\", format='csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
